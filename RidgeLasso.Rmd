---
title: "Exam_RidgeLasso"
output: html_document
date: "2023-11-27"
---
---
title: "RidgeRegressionAndLasso"
output: html_document
date: "2023-11-24"
---

```{r packages}
#install.packages("ISLR2")
#install.packages("glmnet")
library(ISLR2)
library(glmnet)
```


```{r View data1}
#data1 <- read.csv("US-pumpkins.csv", header = TRUE)
data(Hitters)
data1 <- Hitters
View(data1)
```

```{r Missing values}
sum(is.na(data1$Salary))
data1 <- na.omit(data1)
```

#####################################################################
Ridge Regression

If alpha=0 then a ridge regression model is fit, and if alpha=1
then a lasso model is fit. We first fit a ridge regression model.
```{r START Ridge Regression - alpha is 0}
library(glmnet)
x <- model.matrix(Salary ~ ., data1)[, -1] #The model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.
y <- data1$Salary
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
```
Associated with each value of λ is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by coef(). In this case, it is a 20×100
matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of λ).
glmnet() function performs ridge regression for an automatically selected range of λ values. However, here we have chosen to implement the function over a grid of values ranging from λ = 10^10 to λ = 10^−2, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of λ that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.

We expect the coefficient estimates to be much smaller, in terms of ℓ2 norm, when a large value of λ is used, as compared to when a small value of λ is used. These are the coefficients when λ = 11498, along with their ℓ2 norm
```{r lambda: 11498 (input50)}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```
In contrast, here are the coefficients when λ = 705, along with their ℓ2 norm. Note the much larger ℓ2 norm of the coefficients associated with this smaller value of λ.
```{r lambda: 705 (input60)}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```
We can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of λ, say 50
```{r PredictFunction given lambda}
predict(ridge.mod, s=50, type = "coefficients")[1:20,]
```
We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between 1 and n; these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.
We first set a random seed so that the results obtained will be reproducible.
```{r Creating train/testset}
set.seed(1)
data_train <- sample(1:nrow(x), nrow(x) / 2)
data_test <- (-data_train)
y.test <- y[data_test]
```
Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using λ = 4. Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
```{r Creating testset MSE}
ridge.mod <- glmnet(x[data_train, ], y[data_train], 
                    alpha=0, 
                    lambda = grid, 
                    thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx = x[data_test, ])
mean((ridge.pred - y.test)^2)
```
The test MSE is 142,199. Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:
```{r testset MSE with simple intercept}
mean((mean(y[data_train]) - y.test)^2)
#We could also get the same result by fitting a ridge regression model with a very large value of λ. Note that 1e10 means 10^10.
ridge.pred <- predict(ridge.mod, s=1e10, newx=x[data_test,]) #the lambda value in this function is s=10^10=1e10
mean((ridge.pred - y.test)^2)
```
So fitting a ridge regression model with λ = 4 leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with λ = 4 instead of just performing OLS regression. Recall that OLS is simply ridge regression with λ = 0.
```{r showing ridge:lambda0 equals lm:lambda0}
ridge.pred <- predict(ridge.mod, s=0, newx = x[data_test,], exact = T, x = x[data_train,], y=y[data_train]) #  for glmnet() to yield the exact least squares coefficients when λ = 0, we use the argument exact = T when calling the predict() function
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = data_train)
predict(ridge.mod, s=0, exact = T, type = "coefficients", x=x[data_train,], y=y[data_train])[1:20,]
```
In general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.

In general, instead of arbitrarily choosing λ = 4, it would be better to use cross-validation to choose the tuning parameter λ. We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.
```{r FOR REAL: CV Ridge - get bestlam + MSE plot}
set.seed(1)
cv.out <- cv.glmnet(x[data_train,], y[data_train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#The best Lambda-value is 326.0828
```
Therefore, we see that the value of λ that results in the smallest crossvalidation error is 326. What is the test MSE associated with this value of λ?
The test MSE is 142199 for lambda=4, which was arbitrary chosen.
```{r TestMSE by bestlam}
ridge.pred <- predict(ridge.mod, s=bestlam, newx = x[data_test,])
mean((ridge.pred - y.test)^2)
#The TestMSE = 139856.6
```
This represents a further improvement over the test MSE that we got using λ = 4. Finally, we refit our ridge regression model on the full data set, using the value of λ chosen by cross-validation, and examine the coefficient estimates
```{r Examine coef estimates on full dataset}
out <- glmnet(x,y,alpha = 0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```
As expected, none of the coefficients are zero—ridge regression does not perform variable selection! We will experience with Lasso, that Lasso-Regression does perform variable selection.

#####################################################################
Lasso
#####

ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. this time we use the argument alpha=1
```{r START Lasso Regression}
lasso.mod <- glmnet(x[data_train,], y[data_train], alpha=1, lambda=grid)
plot(lasso.mod)
```

We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.
```{r testMSE + MSEplot + bestlam}
set.seed(1)
cv.out.lasso <- cv.glmnet(x[data_train,], y[data_train], alpha=1)
plot(cv.out.lasso)
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
lasso.pred <- predict(lasso.mod, s=bestlam.lasso, newx=x[data_test,])
mean((lasso.pred - y.test)^2)
#The best Lambda-value is 9.286955
#The best testMSE is 143673.6
```
This is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with λ chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse (sparsommelig). Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with λ chosen by cross-validation contains only eleven variables.
```{r Coef by lasso incl 0-estimates}
lasso_full <- glmnet(x,y,alpha=1,lambda=grid)
lasso.coef <- predict(lasso_full,type="coefficients", s=bestlam.lasso)[1:20,]
lasso.coef
```

```{r Coef by lasso excl 0-estimates}
lasso.coef[lasso.coef !=0]
#Shown variables which have not been skrinked to 0 - having the optimal lambda value.
```
