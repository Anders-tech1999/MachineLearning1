---
title: "InClass MonteCarlo"
output: html_document
date: "2024-01-03"
---
This exercise set deals with various questions in the broad topic of model selection. In particular,

Exercise 1: Model Selection with Simulation Study

The purpose of this exercise is to develop a better understanding of model selection criteria and their relative comparisons in a controlled environment with Monte Carlo simulations.
Preliminaries
A Monte Carlo simulation is an experiment that repeatedly generates random variables to conduct some statistical analysis. It is a simple way to obtain a representative distribution of a result for a specific, known process. Although it is not a sufficient proof to demonstrate the general validity of a theory, it is a very useful tool to show specific results, disprove a theory, or provide illustrations. The general structure for a Monte Carlo simulation works as follows:

1. Setup parameters and objects required to store results.
2. Simulate from a known (random) process M times. In each simulation step:
(i) Freshly generate the data according to the known process.
(ii) Conduct the analysis of interest.
(iii) Store the result.
3. Evaluate the results over all M iterations (often averages).

The larger M, the more accurate results tend to be. Depending on the problem of interest, you might require larger or smaller M for convergence of your simulation results.
In many simulations, we require random variable generators in step 2.(i). In this problem set, for example, you need to simulate normally distributed random variables. To do so, you can use rnorm() in R. For example to generate 100 standard normally distributed random variables you can write:

  example <- rnorm(100)
  
If you like to generate normally distributed random variables with some mean "mu" and standard deviation "sigma", you can do so by writing

  example <- rnorm(100,mean = mu,sd = sigma)
  
Other popular distributions for simulation include, but are not limited to, uniform runif(), binomial rbinom(), or t-distribution rt().

List of Questions in Exercise 1
We are interested in the relationship between outcome variable y and predictors x1, x2, . . . , x50.
Let n = 100. Suppose that the true model is given by

y = SUM(5up, p=1down) xp*βp + ε, (Kig doc)

where β1 = β2 = . . . = β5 = 0.05. Hence, the true values of the intercept and β6, . . . , β50 are zero.

Suppose that you have n = 100 observations. For each observation i = 1, . . . , n, you observe a vector of 50 predictors x1,i, x2,i, . . . , x50,i that are independently generated from a standard normal distribution. Each yi is generated following (1) such that εi is also standard normally distributed.
The task is to write a Monte Carlo simulation with M = 1000 replications that reports the probability of choosing the true model with 5 predictors over a model with p = 6, 7, 8, . . . , 50 predictors according to adjusted R2, BIC, and AIC. You may proceed along the following steps:

1. Define 3 matrices of dimension M×45 called select.adj.r2, select.BIC, and select.AIC to store the model selection results.
```{r 3 matrix - AjdR2 BIC AIC}
M <- 1000
beta <- 0.05
n <- 100
p.grid <- seq(6,50) 
select.adj.r2 <- matrix(NA,M,45) 
select.BIC <- matrix(NA,M,45) 
select.AIC <- matrix(NA,M, 45) # matrix with M (1000) rows and 45 columns, filled with NA values. 
```

2. Write a for–loop for m = 1, 2, . . . , M. Within each iteration:
(i) Simulate a new matrix with n × 50 predictors and a new vector of n error terms ε.

(ii) Generate the outcome vector y = (y1, . . . , yn) according to model (1). Note: Do not write another loop here.

(iii) Estimate the true linear regression model that only includes predictors 1, 2, . . . , 5. Save its Adjusted R2 , AIC, and BIC.

(iv) Run another for–loop for p = 6, 7, . . . , 50. For each p:
  (a) Estimate the linear regression model with p predictors,       including the 5 truly relevant predictors.
  (b) Compare the Adjusted R2, AIC, and BIC of the model with     p predictors to the one using only 5 predictors. For each     criterion, save in the matrices from step 1) whether this     criterion suggests to pick the true model over the            alternative model with p predictors.

```{r FOR loop}
for (m in seq(1,M)){
  #report progress (1 = complete)
  print(m/M)
  #simulate TRUE data-generating process from Equation (1)
  x <- matrix(rnorm(n*50),n,50)
  y <- x[,1:5]%*%rep(beta,5) + rnorm(n,0,1) #here %*% is matrix multiplication.
  #estimate model using the true 5 predictors
  mod_true <- lm(y~x[,1:5])
  #store MS criterion for largest model
  adj.r2.true <- summary(mod_true)$adj.r.squared
  bic.true <- BIC(mod_true)
  aic.true <- AIC(mod_true)
  
  #loop over models of size from 6,7,...,50 predictors
  for (p in p.grid){
    #estimate model with p predictors (including the 5 true ones)
    mod.p <- lm(y~x[,1:p])
    #save model selection decision based on all 3 criteria
    select.adj.r2[m,p-5] <- (adj.r2.true > summary(mod.p)$adj.r.squared) #R2 higher = better
    select.BIC[m,p-5] <- (bic.true < BIC(mod.p)) #BIC lower = better
    select.AIC[m,p-5] <- (aic.true < AIC(mod.p)) #AIC lower = better
  }
  
}
```

3. Calculate the share of correctly picking the true model over a model with p predictors along the grid p = 6, . . . , 50 according to the 3 model selection criteria. Also provide a line plot with the same information. Interpret your results in relation to the theoretical justifications of the model selection criteria. What do you conclude?
Note: You may benefit from the commands summary(lm)$adj.r.squared, AIC(lm) and BIC(lm) in parts 2.(iii) and 2.(iv) given regression object lm.
```{r 1.3 share of correct pick of true model}
# Exercise 1.3 report share of correct selection of true model
colMeans(select.adj.r2)
colMeans(select.AIC)
colMeans(select.BIC)

#plot results
##It is possible to plot these lines by using the built-in plot function.

library(ggplot2)
library(reshape2)
plot.data <- data.frame("Degrees.of.Freedom"=p.grid,
                        "Adjusted.R2"=colMeans(select.adj.r2),
                        "BIC"=colMeans(select.BIC),
                        "AIC"=colMeans(select.AIC))

melted = melt(plot.data, id.vars="Degrees.of.Freedom")
colnames(melted)[2] <- "criterion"
ggplot() + geom_line(data=melted, 
                     aes(x=Degrees.of.Freedom, y=value, group=criterion, color=criterion))

## You may also do this plot simply by plot function as below, uncomment it if you want to run
#plot(p.grid, colMeans(select.adj.r2) , type="l", col="red", xlab="Degrees of Freedom", 
#     ylab="Probabilities", ylim=c(0.4,1.1))
#lines(p.grid, colMeans(select.AIC), type="l", col="blue")
#lines(p.grid, colMeans(select.BIC), type="l", col="green")

#interpretation as in lecture:
#Here we have a true model. We know that BIC is consistent, 
#so should select true model with probability close to 1.
#AIC and R2 are inconsistent. R2 actually converges to random model selection (value = 50).
#AIC is better but is aimed at low risk prediction and not consistent model selection and 
#thus the probabilities do not reach 1. 

```

EXERCISE 2  Model Selection in Classification
The purpose of this problem set is to develop a better understanding of model selection techniques with a binary dependent variable in an empirical example. In particular, you will learn how to use model selection in logistic regression, compare their estimates, and evaluate the methods based on the prediction of stock market movements.
This exercise is based on Smarket data of ISLR2 library. The goal is to predict upward (1) or downward (0) movement of the stock market next day, i.e. direction. For more information on this dataset, please read Ch. 4.7.1 of ISLR2 book.

1. Save the Smarket data in a separate data frame. Remove the Today variable from the data as it cannot be used for forecasting. Recode the dependent variable Direction into a numeric (0/1). Split data in years 2001 – 2004 into training set and the data in the year 2005 into test set Afterwards, drop the Year variable. You should be left with 6 predictors
```{r Load SMarket}
library(ISLR2)
names(Smarket)
dim(Smarket)
data1 <- Smarket
```

```{r estimate logit / LDA Multi LR}
#set outcome numeric
data1$Direction <- (data1$Direction == "Up")*1 #This converts Up/Down to 0/1

#split into training and test set by year for prediction. Remove year and Today (cols 1 and 8).
data_train <- data1[data1[,"Year"]!=2005,-c(1,8)]
data_test <- data1[data1[,"Year"]==2005,-c(1,8)]
```

2. Run the following models for estimating the probability of upward/downward movement of the market:
(i) Logistic regression using all predictors.
(ii) Logistic regression using AIC based model selection (among all sub-models).
Compare coefficient estimates. Give an economic interpretation to AIC-optimal model (ii).
```{r logi Full Model}
#This is logit with all variables
mod.logit <- glm(Direction ~., family = binomial, data= data_train)

#Unfortunately, regsubsets() function is for linear models and does not support logit.
# bestglm package does regsubsets kind of things for logit and other families.
#install.packages("bestglm")
library(bestglm)
#bestglm function requires input in stacked X y form where the dependent variable is the last column.
#so we set a separate data frame for this function.
train.bglm <- data_train[, c("Lag1", "Lag2", "Lag3", 
                          "Lag4", "Lag5", 
                          "Volume", "Direction")]
#now we can apply this
mod.logit.aic <- bestglm(train.bglm, IC ="AIC"
                         ,family=binomial, 
                         method = "exhaustive")
#compare coefficients
mod.logit$coef
mod.logit.aic$BestModel$coef

#logit model is a full model, while AIC optimal model is just an intercept only model. So we can't compare more than this.
#AIC model predicts: Market goes up with 50.8016%,  use 1/(1+exp(-beta)) formula.
#This is independent of previous information, and 
#hence no value in looking at return history or volume. 
```


3. Predict the probabilities of upward movement on the test set for both methods. Are the predictions similar?
```{r Proba Logit + AIC-model}
#probabilities and extract probabilities and class prediction
p.logit <- predict(mod.logit, 
                   newdata = data_test, 
                   type = "response")

p.aic <- predict(mod.logit.aic$BestModel, 
                 newdata = data_test, 
                 type = "response")
#compare:
cbind(p.logit,p.aic) 
```


4. How many days could you correctly predict the movement of the market with both method? Do these methods can predict at least 50% correct (random guessing)?

Note: Unfortunately, regsubsets() function that we used for subset selection doesn’t support logistic regression. Instead, you may use bestglm package with the following R command

  bestglm( X y, IC = ”AIC”,family=binomial, method =            ”exhaustive”).

Note that the variables has to stacked so that the dependent variable is the last column. Saving above regression object as mod.aic, the best model can be extracted from mod.aic$BestModel. For more details, please read the function documentation. 
```{r Accuracy}
y_test <- data_test$Direction
y_logit <- (p.logit > 0.5)
y_aic <- (p.aic > 0.5) #always guesses market goes up

c(sum(y_test == y_logit),
  sum(y_test == y_aic))

#Correctly predicted days:
#AIC(141) > logit(121) out of 252
#logit does worse than random guessing.
```
